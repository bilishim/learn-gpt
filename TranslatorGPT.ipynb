{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUUSjanLcPID"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"opus100\", \"en-tr\", split=\"train\") #HuggingFace API anahtarÄ±nÄ± Colab Gizli Anahtarlar alanÄ±na giriyoruz\n",
        "dataset = dataset.select(range(500))  # Ã–rneÄŸin ilk 500 Ã¶rneÄŸi al\n",
        "\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "print(len(train_data))\n",
        "\n",
        "for i in range(10):\n",
        "    example = train_data[i]\n",
        "    print(example)\n"
      ],
      "metadata": {
        "id": "rg4mBLT6nmA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(example[\"translation\"][\"tr\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(example[\"translation\"][\"en\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_data = train_data.map(preprocess, remove_columns=[\"translation\"])\n",
        "test_data = test_data.map(preprocess, remove_columns=[\"translation\"])\n"
      ],
      "metadata": {
        "id": "a6x7gbldov-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch])\n",
        "    return input_ids, labels\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "91_kg3Mno2lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslateGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, 64, embed_dim))\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=512, dropout=0.1, batch_first=True),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=512, dropout=0.1, batch_first=True),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src) + self.pos_encoding[:, :src.size(1), :]\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_encoding[:, :tgt.size(1), :]\n",
        "        memory = self.encoder(src_emb)\n",
        "        output = self.decoder(tgt_emb, memory)\n",
        "        return self.output_layer(output)\n"
      ],
      "metadata": {
        "id": "IZEzYVz_sFLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TranslateGPT(vocab_size=tokenizer.vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Ä°lk 5 Ã¶rneÄŸi al\n",
        "mini_data = train_data.select(range(5))\n",
        "\n",
        "mini_loader = DataLoader(mini_data, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# EÄŸitim: 100 epoch (kÃ¼Ã§Ã¼k veri)\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_ids, labels in mini_loader:\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "        tgt_input = labels[:, :-1]\n",
        "        tgt_output = labels[:, 1:]\n",
        "\n",
        "        logits = model(input_ids, tgt_input)\n",
        "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"[Mini] Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "n0T3ODBisKAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(model, input_text, max_len=64, verbose=True):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=max_len, truncation=True)\n",
        "    src = tokens[\"input_ids\"].to(device)\n",
        "\n",
        "    # BaÅŸlangÄ±Ã§ tokenÄ± (CLS veya BOS, varsa BOS kullan)\n",
        "    bos_token = tokenizer.bos_token_id or tokenizer.cls_token_id\n",
        "    sep_token = tokenizer.sep_token_id\n",
        "    pad_token = tokenizer.pad_token_id\n",
        "\n",
        "    tgt = torch.tensor([[bos_token]], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output = model(src, tgt)\n",
        "        next_token = torch.argmax(output[:, -1, :], dim=-1, keepdim=True)\n",
        "\n",
        "        if next_token.item() in [sep_token, pad_token]:\n",
        "            break  # optional: veya devam et\n",
        "\n",
        "        tgt = torch.cat([tgt, next_token], dim=1)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Output token ids :\", tgt[0].tolist())\n",
        "        print(\"Raw decoded      :\", tokenizer.decode(tgt[0], skip_special_tokens=False))\n",
        "        print(\"Cleaned decoded  :\", tokenizer.decode(tgt[0], skip_special_tokens=True))\n",
        "\n",
        "    return tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xBc48iLTuAti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ã–rnek\n",
        "sample = \"- Onu Ã¶ldÃ¼rmenize gerek yok, Bay Howie.\"\n",
        "print(\"Girdi:\", sample)\n",
        "translated = translate_text(model, sample)\n",
        "print(\"Ã‡eviri:\", translated)"
      ],
      "metadata": {
        "id": "2anqYxi4zlSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION MEKANÄ°ZMASI NE SAÄLADI\n",
        "\n",
        "ğŸ¯ Odaklanma\tModel, hangi kelimenin hangi kelimeyle iliÅŸkili olduÄŸunu Ã¶ÄŸrenir (Ã¶rneÄŸin \"teÅŸekkÃ¼rler\" â†’ \"thank you\")\n",
        "\n",
        "ğŸ§© ParÃ§alÄ± anlam\tTÃ¼m cÃ¼mleyi tek bir gizli vektÃ¶re sÄ±kÄ±ÅŸtÄ±rmak yerine, her bir kelimenin baÄŸlamÄ±na Ã¶zgÃ¼ temsilini tutar\n",
        "\n",
        "ğŸ”€ DoÄŸal kelime sÄ±rasÄ±\tDiller arasÄ± kelime sÄ±ralarÄ± farklÄ± olsa da, attention mekanizmasÄ± bu farklÄ±lÄ±klarÄ± Ã§Ã¶zebilir\n",
        "\n",
        "ğŸ” Uzun baÄŸÄ±mlÄ±lÄ±klar\tCÃ¼mlenin baÅŸÄ±ndaki bir kelimeyle sonundaki kelime arasÄ±ndaki iliÅŸkileri kurabilir"
      ],
      "metadata": {
        "id": "2gr4ibgMEkFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EÄÄ°TÄ°MÄ°N AMACI NEDÄ°R, LOOKUP TABLE KULLANILAMAZ MIYDI?\n",
        "\n",
        "EÄŸitim sÄ±rasÄ±nda:\n",
        "\n",
        "Model sadece ezberlemez, genelleÅŸtirir\n",
        "\n",
        "AynÄ± yapÄ±yÄ± taÅŸÄ±yan ama yeni kelimelerle gelen cÃ¼mleleri Ã§evirmeyi Ã¶ÄŸrenir\n",
        "\n",
        "Ã–rneÄŸin:\n",
        "\n",
        "\n",
        "GÃ¶rmedim â†’ I didn't see\n",
        "DuymadÄ±m â†’ I didn't hear\n",
        "gibi Ã¶rneklerden sonra, model:\n",
        "\n",
        "AnlamadÄ±m â†’ I didn't understand\n",
        "gibi Ã§eviriyi daha Ã¶nce gÃ¶rmemiÅŸ olsa bile Ã§Ä±karabilir."
      ],
      "metadata": {
        "id": "rE-Q2V10EQqC"
      }
    }
  ]
}
