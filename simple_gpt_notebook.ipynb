{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "Now let's train our model on the simple example to see if it can learn patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Simple training example\n",
    "# In practice, you'd train on much more data for much longer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Create training data: predict next character\n",
    "# If input is \"hello\", target is \"ello \" (shifted by one position)\n",
    "input_seq = input_ids[:, :-1]   # All tokens except the last\n",
    "target_seq = input_ids[:, 1:]   # All tokens except the first\n",
    "\n",
    "print(f\"Training input shape: {input_seq.shape}\")\n",
    "print(f\"Training target shape: {target_seq.shape}\")\n",
    "print(f\"Input sequence: {decode(input_seq[0].tolist())}\")\n",
    "print(f\"Target sequence: {decode(target_seq[0].tolist())}\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few steps (real training would be thousands/millions of steps)\n",
    "losses = []\n",
    "for step in range(100):\n",
    "    loss = train_step(model, optimizer, input_seq, target_seq)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step:3d}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "print(f\"Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"The decreasing loss shows the model is learning to predict the next character!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing the Trained Model\n",
    "\n",
    "Let's see how the model performs after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Generate after some training (should be slightly better)\n",
    "print(\"=== Generation After Training ===\")\n",
    "print(\"Testing different prompts and generation parameters:\\n\")\n",
    "\n",
    "# Test with original prompt\n",
    "generated = model.generate(input_ids[:, :5], max_new_tokens=20, temperature=0.8)\n",
    "generated_text = decode(generated[0].tolist())\n",
    "prompt_text = decode(input_ids[0, :5].tolist())\n",
    "print(f\"Prompt: '{prompt_text}'\")\n",
    "print(f\"Generated: '{generated_text}'\")\n",
    "print()\n",
    "\n",
    "# Test with different temperatures\n",
    "print(\"Testing different temperatures:\")\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    generated = model.generate(input_ids[:, :3], max_new_tokens=15, temperature=temp)\n",
    "    generated_text = decode(generated[0].tolist())\n",
    "    print(f\"Temperature {temp}: '{generated_text}'\")\n",
    "\n",
    "print(\"\\nNote: Lower temperature = more deterministic, Higher temperature = more random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding Attention\n",
    "\n",
    "Let's visualize what the model has learned by examining attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract attention weights\n",
    "def get_attention_weights(model, input_ids, layer_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    Extract attention weights from a specific layer and head.\n",
    "    This shows us what the model is \"paying attention\" to.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # We need to modify the forward pass to capture attention weights\n",
    "    # For simplicity, let's manually compute attention for the first layer\n",
    "    with torch.no_grad():\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # Get embeddings\n",
    "        pos_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        pos_ids = pos_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \n",
    "        token_embeds = model.token_embedding(input_ids)\n",
    "        pos_embeds = model.position_embedding(pos_ids)\n",
    "        x = token_embeds + pos_embeds\n",
    "        \n",
    "        # Get attention from first layer\n",
    "        attention_layer = model.blocks[layer_idx].attention\n",
    "        x_norm = model.blocks[layer_idx].norm1(x)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = attention_layer.w_q(x_norm).view(batch_size, seq_len, attention_layer.n_heads, attention_layer.d_k).transpose(1, 2)\n",
    "        K = attention_layer.w_k(x_norm).view(batch_size, seq_len, attention_layer.n_heads, attention_layer.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(attention_layer.d_k)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0) == 0, -1e9)\n",
    "        \n",
    "        # Get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        return attn_weights[0, head_idx].cpu().numpy()  # Return specific head\n",
    "\n",
    "# Visualize attention\n",
    "test_text = \"hello world\"\n",
    "test_tokens = encode(test_text)\n",
    "test_input = torch.tensor([test_tokens], dtype=torch.long)\n",
    "\n",
    "attention_matrix = get_attention_weights(model, test_input, layer_idx=0, head_idx=0)\n",
    "\n",
    "# Plot attention heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_matrix, cmap='Blues')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.title('Attention Weights (Layer 0, Head 0)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "\n",
    "# Add token labels\n",
    "tokens_str = [decode([t]) for t in test_tokens]\n",
    "plt.xticks(range(len(tokens_str)), tokens_str)\n",
    "plt.yticks(range(len(tokens_str)), tokens_str)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This heatmap shows how much attention each position pays to each other position.\")\n",
    "print(\"Brighter colors = higher attention weights.\")\n",
    "print(\"Notice the triangular pattern due to causal masking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Analysis and Next Steps\n",
    "\n",
    "Let's analyze what we've learned and discuss improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance on the training text\n",
    "print(\"=== Model Analysis ===\")\n",
    "print()\n",
    "\n",
    "# Check if model memorized the training sequence\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(input_ids)\n",
    "    \n",
    "    # Get predicted tokens (most likely next token at each position)\n",
    "    predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    print(\"Training sequence analysis:\")\n",
    "    print(f\"Original:  {text}\")\n",
    "    print(f\"Predicted: {decode(predicted_tokens[0].tolist())}\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    target_tokens = input_ids[:, 1:]  # Shifted by one\n",
    "    pred_tokens = predicted_tokens[:, :-1]  # Remove last prediction\n",
    "    accuracy = (pred_tokens == target_tokens).float().mean().item()\n",
    "    \n",
    "    print(f\"\\nNext-token prediction accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    if accuracy > 0.8:\n",
    "        print(\"âœ… Model has learned the training sequence well!\")\n",
    "    elif accuracy > 0.5:\n",
    "        print(\"âš ï¸  Model has partially learned the sequence.\")\n",
    "    else:\n",
    "        print(\"âŒ Model needs more training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generalization with similar but different text\n",
    "print(\"\\n=== Generalization Test ===\")\n",
    "test_phrases = [\n",
    "    \"hello\",\n",
    "    \"world\",\n",
    "    \"simple\",\n",
    "    \"model\",\n",
    "]\n",
    "\n",
    "for phrase in test_phrases:\n",
    "    phrase_tokens = encode(phrase)\n",
    "    phrase_input = torch.tensor([phrase_tokens], dtype=torch.long)\n",
    "    \n",
    "    generated = model.generate(phrase_input, max_new_tokens=10, temperature=0.7)\n",
    "    generated_text = decode(generated[0].tolist())\n",
    "    \n",
    "    print(f\"'{phrase}' -> '{generated_text}'\")\n",
    "\n",
    "print(\"\\nNote: Since we only trained on one sentence, generalization will be limited.\")\n",
    "print(\"Real GPT models train on billions of tokens from diverse text sources!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps and Improvements\n",
    "\n",
    "Here are ways to extend and improve this simple GPT implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Next Steps to Improve Your GPT:\")\n",
    "print()\n",
    "print(\"1. ðŸ“š More Training Data:\")\n",
    "print(\"   - Use larger text datasets (books, articles, web text)\")\n",
    "print(\"   - Implement proper data loading and batching\")\n",
    "print(\"   - Add data preprocessing and cleaning\")\n",
    "print()\n",
    "print(\"2. ðŸ”¤ Better Tokenization:\")\n",
    "print(\"   - Implement Byte-Pair Encoding (BPE)\")\n",
    "print(\"   - Use subword tokenization\")\n",
    "print(\"   - Handle out-of-vocabulary words better\")\n",
    "print()\n",
    "print(\"3. ðŸ—ï¸ Architecture Improvements:\")\n",
    "print(\"   - Add more layers and larger hidden dimensions\")\n",
    "print(\"   - Experiment with different attention mechanisms\")\n",
    "print(\"   - Try different positional encodings (RoPE, ALiBi)\")\n",
    "print(\"   - Add gradient clipping and better optimization\")\n",
    "print()\n",
    "print(\"4. ðŸ“Š Training Enhancements:\")\n",
    "print(\"   - Implement learning rate scheduling\")\n",
    "print(\"   - Add validation and early stopping\")\n",
    "print(\"   - Use mixed precision training\")\n",
    "print(\"   - Implement gradient accumulation\")\n",
    "print()\n",
    "print(\"5. ðŸŽ¯ Advanced Features:\")\n",
    "print(\"   - Add instruction tuning capabilities\")\n",
    "print(\"   - Implement RLHF (Reinforcement Learning from Human Feedback)\")\n",
    "print(\"   - Add safety filtering and alignment\")\n",
    "print(\"   - Experiment with different sampling strategies\")\n",
    "print()\n",
    "print(\"6. ðŸ” Analysis Tools:\")\n",
    "print(\"   - Visualize attention patterns across layers\")\n",
    "print(\"   - Analyze what the model has learned\")\n",
    "print(\"   - Implement perplexity evaluation\")\n",
    "print(\"   - Add interpretability tools\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ“ Congratulations! You now understand the core concepts of GPT!\")\n",
    "print(\"ðŸ§  You've learned about attention, transformers, and autoregressive generation.\")\n",
    "print(\"ðŸš€ This foundation will help you understand and work with modern LLMs!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we implemented a simplified GPT model from scratch and learned about:\n",
    "\n",
    "- **Multi-Head Attention**: How models attend to different parts of the input\n",
    "- **Transformer Architecture**: Building blocks of modern language models\n",
    "- **Autoregressive Generation**: How text is generated one token at a time\n",
    "- **Training Process**: Next-token prediction objective\n",
    "- **Attention Visualization**: Understanding what the model focuses on\n",
    "\n",
    "While this is a simplified version, it contains all the essential components of GPT and provides a solid foundation for understanding how large language models work!\n",
    "\n",
    "The model might start to show some patterns after training on this small example, but real language modeling requires much more data and training time. Modern GPT models like GPT-4 are trained on billions of tokens from diverse text sources and have billions of parameters!\n",
    "\n",
    "Happy learning! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
   "source": [
    "# Simple GPT Implementation\n",
    "\n",
    "This notebook implements a simplified version of GPT (Generative Pre-trained Transformer) to help understand how Large Language Models work.\n",
    "\n",
    "## Overview\n",
    "- **Multi-Head Attention**: The core mechanism for attending to different parts of input\n",
    "- **Transformer Blocks**: Building blocks that combine attention + feed-forward networks\n",
    "- **Autoregressive Generation**: How the model generates text one token at a time\n",
    "- **Training**: Simple next-token prediction training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention\n",
    "\n",
    "The core of transformer models. This allows the model to attend to information from different representation subspaces at different positions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism - the core of transformer models.\n",
    "    \n",
    "    This allows the model to attend to information from different representation \n",
    "    subspaces at different positions simultaneously. Think of it as having multiple\n",
    "    \"attention heads\" that can focus on different aspects of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        # d_model: dimension of the model (embedding size)\n",
    "        # n_heads: number of parallel attention heads\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        # Each head will have dimension d_k = d_model / n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Queries, Keys, Values, and Output\n",
    "        # These transform the input into different representation spaces\n",
    "        self.w_q = nn.Linear(d_model, d_model)  # Query projection\n",
    "        self.w_k = nn.Linear(d_model, d_model)  # Key projection  \n",
    "        self.w_v = nn.Linear(d_model, d_model)  # Value projection\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # Final output projection\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask to prevent attending to certain positions\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Step 1: Create Query, Key, Value matrices for all heads\n",
    "        # Transform input through linear layers, then reshape for multiple heads\n",
    "        # Original shape: (batch_size, seq_len, d_model)\n",
    "        # After linear: (batch_size, seq_len, d_model) \n",
    "        # After view: (batch_size, seq_len, n_heads, d_k)\n",
    "        # After transpose: (batch_size, n_heads, seq_len, d_k)\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 2: Scaled dot-product attention\n",
    "        # Calculate attention scores: how much each position should attend to every other position\n",
    "        # Q @ K^T gives us a (seq_len, seq_len) matrix of attention scores for each head\n",
    "        # We scale by sqrt(d_k) to prevent softmax from becoming too peaked\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Step 3: Apply causal mask (for autoregressive generation)\n",
    "        # This prevents the model from \"cheating\" by looking at future tokens\n",
    "        # We set future positions to -infinity so they become 0 after softmax\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Step 4: Convert scores to probabilities and apply to values\n",
    "        # Softmax normalizes the attention scores to probabilities (sum to 1)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        # Multiply attention weights with values to get the attended output\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Step 5: Concatenate heads and project back to original dimension\n",
    "        # Transpose back: (batch_size, n_heads, seq_len, d_k) -> (batch_size, seq_len, n_heads, d_k)\n",
    "        # Reshape to concatenate heads: (batch_size, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear transformation to mix information from all heads\n",
    "        return self.w_o(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network\n",
    "\n",
    "A simple 2-layer MLP applied to each position independently. Provides non-linear transformation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    This is a simple 2-layer MLP applied to each position independently.\n",
    "    It provides the model with non-linear transformation capabilities.\n",
    "    The typical pattern is: Linear -> ReLU -> Dropout -> Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # d_ff is typically 4 * d_model (expansion factor of 4)\n",
    "        # First layer expands the dimension, second layer projects back\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)    # Expand: d_model -> d_ff\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)    # Contract: d_ff -> d_model\n",
    "        self.dropout = nn.Dropout(0.1)             # Regularization to prevent overfitting\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply feed-forward transformation to each position.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Expand -> Activate -> Regularize -> Contract\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Block\n",
    "\n",
    "The fundamental building block that combines attention and feed-forward networks with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block consisting of:\n",
    "    1. Multi-head self-attention\n",
    "    2. Feed-forward network\n",
    "    3. Residual connections around both\n",
    "    4. Layer normalization before each sub-layer (Pre-LN architecture)\n",
    "    \n",
    "    This is the fundamental building block that gets stacked to create deep transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        # Layer normalization for stabilizing training\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Before attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # Before feed-forward\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "        Uses Pre-LN architecture: LayerNorm -> SubLayer -> Residual\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        # Pre-LN: normalize first, then apply attention, then add residual\n",
    "        attn_output = self.attention(self.norm1(x), mask)\n",
    "        x = x + attn_output  # Residual connection helps with gradient flow\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        # Same pattern: normalize, transform, add residual\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + ff_output    # Another residual connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple GPT Model\n",
    "\n",
    "The main GPT model that combines all components into a complete language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified GPT (Generative Pre-trained Transformer) model.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Token embeddings (convert token IDs to vectors)\n",
    "    2. Positional embeddings (give model sense of position/order)\n",
    "    3. Stack of transformer blocks (the main computation)\n",
    "    4. Final layer norm + linear projection to vocabulary\n",
    "    \n",
    "    This is a decoder-only transformer that generates text autoregressively\n",
    "    (one token at a time, using previously generated tokens as context).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6, max_seq_len=1024):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layers: convert discrete tokens to continuous vectors\n",
    "        # Token embedding: maps token IDs (0, 1, 2, ...) to dense vectors\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Position embedding: gives model information about token position in sequence\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of transformer blocks - this is where the main computation happens\n",
    "        # Each block can attend to and process information from the entire sequence\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_model * 4)  # d_ff = 4 * d_model is standard\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final processing layers\n",
    "        self.ln_f = nn.LayerNorm(d_model)               # Final layer normalization\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)  # Project to vocabulary size\n",
    "        \n",
    "        # Initialize all weights with appropriate values for stable training\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize model weights for stable training.\n",
    "        \n",
    "        Proper initialization is crucial for transformer training:\n",
    "        - Linear layers: small random values (normal distribution with std=0.02)\n",
    "        - Embeddings: small random values \n",
    "        - Biases: zeros\n",
    "        \n",
    "        This follows the initialization scheme used in GPT models.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialize linear layer weights with small random values\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding weights with small random values\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass through the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tensor of token IDs, shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Predictions for next token at each position, shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # Step 1: Create position indices for positional embeddings\n",
    "        # We need to tell the model what position each token is at\n",
    "        # pos_ids will be [0, 1, 2, ..., seq_len-1] for each sequence in batch\n",
    "        pos_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        pos_ids = pos_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \n",
    "        # Step 2: Get embeddings and combine them\n",
    "        # Token embeddings: convert token IDs to dense vectors\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        # Position embeddings: add positional information\n",
    "        pos_embeds = self.position_embedding(pos_ids)\n",
    "        # Combine: each token gets both its semantic meaning and positional info\n",
    "        x = token_embeds + pos_embeds\n",
    "        \n",
    "        # Step 3: Create causal mask for autoregressive generation\n",
    "        # This ensures each position can only attend to previous positions\n",
    "        # Lower triangular matrix: 1s below diagonal, 0s above\n",
    "        # [[1, 0, 0],\n",
    "        #  [1, 1, 0], \n",
    "        #  [1, 1, 1]]\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "        \n",
    "        # Step 4: Pass through transformer blocks\n",
    "        # Each block refines the representations using attention and feed-forward\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Step 5: Final processing\n",
    "        # Layer norm for stability, then project to vocabulary size\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively (one token at a time).\n",
    "        \n",
    "        This is how GPT actually generates text: start with some prompt tokens,\n",
    "        predict the next token, add it to the sequence, then predict the next one, etc.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Starting tokens, shape (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            temperature: Controls randomness (1.0 = normal, <1.0 = more deterministic, >1.0 = more random)\n",
    "            top_k: If set, only consider the top-k most likely tokens for sampling\n",
    "        \n",
    "        Returns:\n",
    "            Generated sequence including original input_ids\n",
    "        \"\"\"\n",
    "        self.eval()  # Set to evaluation mode (disables dropout, etc.)\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Step 1: Get predictions for the current sequence\n",
    "                # Model outputs logits for next token at each position\n",
    "                logits = self.forward(input_ids)\n",
    "                \n",
    "                # Step 2: Focus on the last token's predictions\n",
    "                # We only care about predicting the next token after the last position\n",
    "                logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n",
    "                \n",
    "                # Step 3: Apply top-k filtering if specified\n",
    "                # This limits sampling to only the k most likely tokens\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    # Set all logits below the k-th highest to negative infinity\n",
    "                    logits[logits < v[:, [-1]]] = -float('inf')\n",
    "                \n",
    "                # Step 4: Sample from the probability distribution\n",
    "                # Convert logits to probabilities using softmax\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # Sample one token from this distribution\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Step 5: Add the new token to our sequence\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                \n",
    "                # Step 6: Stop if we exceed maximum sequence length\n",
    "                if input_ids.size(1) >= self.max_seq_len:\n",
    "                    break\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenizer and Training Utilities\n",
    "\n",
    "Simple character-level tokenizer and training functions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_tokenizer():\n",
    "    \"\"\"\n",
    "    Create a very basic character-level tokenizer for demonstration.\n",
    "    \n",
    "    Real GPT models use more sophisticated tokenizers like BPE (Byte-Pair Encoding),\n",
    "    but this simple version helps understand the concept.\n",
    "    \n",
    "    Returns:\n",
    "        encode: Function to convert text to token IDs\n",
    "        decode: Function to convert token IDs back to text  \n",
    "        vocab_size: Size of the vocabulary\n",
    "    \"\"\"\n",
    "    # Define our vocabulary: lowercase letters, space, and basic punctuation\n",
    "    chars = list(\"abcdefghijklmnopqrstuvwxyz .,!?'\")\n",
    "    vocab_size = len(chars)\n",
    "    \n",
    "    # Create mappings between characters and indices\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}  # 'a' -> 0, 'b' -> 1, etc.\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}  # 0 -> 'a', 1 -> 'b', etc.\n",
    "    \n",
    "    def encode(text):\n",
    "        \"\"\"Convert text string to list of token IDs\"\"\"\n",
    "        return [char_to_idx.get(ch.lower(), 0) for ch in text]  # Unknown chars -> 0\n",
    "    \n",
    "    def decode(indices):\n",
    "        \"\"\"Convert list of token IDs back to text string\"\"\"\n",
    "        return ''.join([idx_to_char.get(idx, '') for idx in indices])\n",
    "    \n",
    "    return encode, decode, vocab_size\n",
    "\n",
    "def train_step(model, optimizer, input_ids, targets):\n",
    "    \"\"\"\n",
    "    Single training step for the model.\n",
    "    \n",
    "    The training objective is to predict the next token given previous tokens.\n",
    "    This is called \"causal language modeling\" or \"autoregressive training\".\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT model\n",
    "        optimizer: Optimizer for updating weights\n",
    "        input_ids: Input token sequence\n",
    "        targets: Target tokens (input_ids shifted by one position)\n",
    "    \n",
    "    Returns:\n",
    "        Loss value for this training step\n",
    "    \"\"\"\n",
    "    model.train()  # Set to training mode (enables dropout, etc.)\n",
    "    optimizer.zero_grad()  # Clear gradients from previous step\n",
    "    \n",
    "    # Forward pass: get model predictions\n",
    "    logits = model(input_ids)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "    # Calculate loss: cross-entropy between predictions and targets\n",
    "    # We flatten the tensors to compute loss over all positions\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Usage and Training\n",
    "\n",
    "Let's create a model and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create tokenizer\n",
    "# This converts between text and numbers that the model can work with\n",
    "encode, decode, vocab_size = create_simple_tokenizer()\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Example encoding: '{encode('hello')}' -> 'hello'\")\n",
    "print(f\"Example decoding: {decode([7, 4, 11, 11, 14])} <- [7, 4, 11, 11, 14]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize model with reasonable parameters for experimentation\n",
    "# Smaller than real GPT models but large enough to learn patterns\n",
    "model = SimpleGPT(vocab_size=vocab_size, d_model=256, n_heads=8, n_layers=4)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare example text and convert to tokens\n",
    "text = \"hello world, this is a simple gpt model.\"\n",
    "tokens = encode(text)  # Convert text to list of integers\n",
    "input_ids = torch.tensor([tokens], dtype=torch.long)  # Convert to tensor\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Input tensor shape: {input_ids.shape}\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    print(f\"Model output shape: {output.shape}\")\n",
    "    print(f\"Output represents logits for {output.shape[-1]} vocabulary tokens at each of {output.shape[1]} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate some text (will be random gibberish before training)\n",
    "# We give it the first 5 tokens and ask it to continue\n",
    "print(\"\\n=== Generation Before Training ===\")\n",
    "generated = model.generate(input_ids[:, :5], max_new_tokens=20)\n",
    "generated_text = decode(generated[0].tolist())\n",
    "print(f\"Prompt: '{decode(input_ids[0, :5].tolist())}'\")\n",
    "print(f\"Generated (untrained): '{generated_text}'\")\n",
    "print(\"Note: This will be random gibberish since the model hasn't been trained yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "